{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data\n",
    "\n",
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# packages needed for creating x \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Z and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "E=[0.2, 2, 3, 5] # environmental factors\n",
    "S=[1,2,3,4]\n",
    "\n",
    "\n",
    "def generate_z_and_y(E,S):\n",
    "    beta_z1 = np.random.normal(0,1)\n",
    "    beta_z2 = np.random.normal(0,1)\n",
    "    beta_z3 = np.random.normal(0,1)\n",
    "    env_y = []\n",
    "    env_z = []\n",
    "\n",
    "    for i in range(len(E)):\n",
    "        # Draw beta\n",
    "        beta1 = np.random.normal(0,1)\n",
    "        beta2 = np.random.normal(0,1)\n",
    "        \n",
    "        # Create Z1\n",
    "        Z1=np.random.normal(beta1*E[i], 1, 1000)\n",
    "        #Z1=np.vstack((Z1, np.array([S[i]]*1000))).reshape(-1,1000).T\n",
    "        \n",
    "        # Create Z2\n",
    "        Z2=np.random.normal(2*beta2*E[i], 2, 1000)\n",
    "        #Z2=np.vstack((Z2, np.array([S[i]]*1000))).reshape(-1,1000).T\n",
    "\n",
    "        # Create Y\n",
    "        Y=np.zeros(1000)\n",
    "\n",
    "        # Create Z3\n",
    "        Z3=np.zeros(1000)\n",
    "        for i in range(len(Z1)):\n",
    "            Y[i] = np.random.normal(beta_z1*Z1[i]+beta_z2*Z2[i], 1)\n",
    "\n",
    "\n",
    "            Z3[i]=np.random.normal(3*beta_z3*Y[i], 1)\n",
    "\n",
    "        env_y.append(Y)\n",
    "        env_z.append(pd.DataFrame(np.vstack((Z1[:], Z2[:], Z3[:])).T))\n",
    "\n",
    "\n",
    "    Z=pd.concat([env_z[0], env_z[1], env_z[2], env_z[3]],axis=1)\n",
    "    Zs=np.zeros((1000, 4, 4))\n",
    "    # categories=[0,1,2,3]\n",
    "    for i in range(len(E)):#categories:\n",
    "        Zs[:,:3,i]=env_z[i]\n",
    "        Zs[:,3,i]=i+1\n",
    "\n",
    "    return Zs, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs, Y = generate_z_and_y(E,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Hyperparameters\n",
    "num_classes = 10\n",
    "num_l1 = 6\n",
    "num_features = 3\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_output):\n",
    "        super(Net, self).__init__()  \n",
    "        # hidden layer\n",
    "        self.W_1 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden, num_features)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
    "        # output layer\n",
    "        self.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_output, num_hidden)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "        # define activation function in constructor\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_features, num_l1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(net(torch.from_numpy(Zs[:,:3,1].astype(\"float32\"))).size())\n",
    "\n",
    "X = net(torch.from_numpy(Zs[:,:3,1].astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformatting Data to Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to export the data to csvs\n",
    "# Will remove after next github push\n",
    "\n",
    "# # turn all data into Dataframes \n",
    "# y_df = pd.DataFrame(Y, columns = ['Y'])\n",
    "# x_df = pd.DataFrame(X)\n",
    "# # zs_df = pd.DataFrame(Zs, columns = [\"Z1\", \"Z2\", \"Z3\", \"Environments\"])\n",
    "# # save data to file \n",
    "# y_df.to_csv('y.csv') \n",
    "# # zs_df.to_csv('zs.csv') \n",
    "# x_df.to_csv('x.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomSyntheticDataset(Dataset):\n",
    "    def __init__(self,X,Zs,Y):\n",
    "        self.X = X\n",
    "        self.Zs = Zs\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get label from Y \n",
    "        label = self.Y[idx,:]\n",
    "        # get Zs for each env't\n",
    "        selected_zs = self.Zs[idx,:,:] \n",
    "        # get X \n",
    "        selected_x = self.X[idx,:]\n",
    "        return selected_x, selected_zs, label\n",
    "\n",
    "    def return_training(self):\n",
    "        return X\n",
    "    \n",
    "    def return_testing(self):\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_instance = CustomSyntheticDataset(X,Zs,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying VAE Exercise Data Import Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "# data imports \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "#classes = [*range(1, len(E), 1)] # environments\n",
    "#def stratified_sampler(labels):\n",
    "#    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "#    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "#    indices = torch.from_numpy(indices)\n",
    "#    return SubsetRandomSampler(indices)\n",
    "\n",
    "dset_train = dataset_instance.return_training()\n",
    "dset_test = dataset_instance.return_testing()\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 100\n",
    "# The loaders perform the actual work\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size)\n",
    "test_loader  = DataLoader(dset_test, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        return self.sample_epsilon() * self.sigma + self.mu \n",
    "        \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        # create a normal distribution to sample from \n",
    "        m = Normal(self.mu, self.sigma)\n",
    "        # get probability of choosing z from that distribution\n",
    "        return m.log_prob(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=6, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "\n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # q_phi(z|x) = N(z | mu(x), sigma(x)),\n",
    "        # mu(x),\n",
    "        # log(sigma(x)) = h_phi(x)`\n",
    "        \n",
    "        # Step 1:\n",
    "        # Define input dimensions -> self.input_shape\n",
    "        # Step 2:\n",
    "        # Define the rest of the encoding architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "             nn.Linear(in_features=self.observation_features, out_features=100),\n",
    "             nn.ReLU(),\n",
    "            # nn.Linear(in_features=1024, out_features=512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(in_features=512, out_features=256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(in_features=256, out_features=128),\n",
    "            # nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=100, out_features=2*latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_theta(x | z) = prod_i B(x_i | g_theta(x))`\n",
    "        \n",
    "        # Step 3:\n",
    "        # Decode from latent space back to X\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(in_features=128, out_features=256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(in_features=256, out_features=512),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(in_features=512, out_features=1024),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features=self.observation_features)\n",
    "        )\n",
    "        \n",
    "        # Step 4:\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | mu(x), sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | mu(x), sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits = self.decoder(z)\n",
    "        #pdb.set_trace()\n",
    "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
    "        return Bernoulli(logits=px_logits, validate_args=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z) # Just assume a standard prior with mean 0 and var 1\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        # Decode\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "#    def sample_from_prior(self, batch_size:int=100):\n",
    "#        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "#        \n",
    "#        # define the prior p(z)\n",
    "#        pz = self.prior(batch_size=batch_size)\n",
    "#        \n",
    "#        # sample the prior \n",
    "#        z = pz.rsample()\n",
    "#        \n",
    "#        # define the observation model p(x|z) = B(x | g(z))\n",
    "#        px = self.observation_model(z)\n",
    "#        \n",
    "#        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "\n",
    "latent_features = 3\n",
    "vae = VariationalAutoencoder(X[0].shape, latent_features)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Variational Inference (ie Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        # Get parameters of the prior and posterior and px and z's\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        # Whats the probability of getting x and z given the estimated distributions\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^beta = E_q [ log p(x|z) ] - beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl # <- your code here\n",
    "        beta_elbo =  log_px - self.beta*kl # <- your code here\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization \n",
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 3\n",
    "vae = VariationalAutoencoder(X[0].shape, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17396/1440491167.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Backup_AI\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Backup_AI\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "## Training Loop \n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    #for x, y in train_loader:\n",
    "    for x in train_loader:\n",
    "    \n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        #x, y = next(iter(test_loader))\n",
    "        x= next(iter(test_loader))\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBOwhy \n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    # make_vae_plots(vae, x, y, outputs, training_data, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee89bb07d3fa5f9c0a8bd127b3056783b076f56308f3ae1a55a1b4c1c97796ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
