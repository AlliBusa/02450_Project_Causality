{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b58351",
   "metadata": {},
   "source": [
    "# VAE and iVAE code is at the bottom #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d194f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathi\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# packages needed for creating x\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Normal\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "from typing import *\n",
    "\n",
    "# data imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# from torchvision.transforms import ToTensor\n",
    "# from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2c52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(x):\n",
    "    \"\"\"Converts input array to pandas dataframe\"\"\"\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "\n",
    "##################################\n",
    "# Generate Zs and Ys\n",
    "\n",
    "\n",
    "def generate_z_and_y(E, samples, three_z=True):\n",
    "    \"\"\"\n",
    "    Takes in a list of means representing different environments and\n",
    "    generates latent variables (z's) and y for that environment\n",
    "\n",
    "    Currently only works for 4 environments and 4 output variables\n",
    "\n",
    "    Args:\n",
    "        E (list): A list of four numbers representing four means of four different environments\n",
    "\n",
    "    Returns:\n",
    "        _envs (nested dictionary): A nested dictionary containing all environments\n",
    "        and latent variables for each \n",
    "            ex: envs.keys() = [0,1] -> contains data for environment 0 and 1\n",
    "                envs[0].keys() = [\"Y\",\"Zs\"] -> Each environment contains a dictionary \n",
    "                with a numpy array of Y values and a numpy array for Z values \n",
    "                The Z values are organized column wise (ie the first column contains the first latent variable)\n",
    "    \"\"\"\n",
    "\n",
    "    beta_z1 = 1  # np.random.normal(0,1)\n",
    "    beta_z2 = 1  # np.random.normal(0,1)\n",
    "    beta_z3 = 1  # np.random.normal(0,1)\n",
    "    \n",
    "    E_choice = np.random.choice(np.arange(0,len(E), 1), samples)\n",
    "    Env = E[E_choice]\n",
    "    \n",
    "    Z1 = np.random.normal(Env, 1, size = samples)\n",
    "    Z2 = np.random.normal(2*Env, 2, size = samples)\n",
    "    Y = np.random.normal(Z1+Z2, 1, size = samples)\n",
    "    Z3 = Y + np.random.normal(0, 1, size=samples)\n",
    "    if three_z==True:\n",
    "        Z = np.stack([Z1, Z2, Z3], axis = 1)\n",
    "    else:\n",
    "        Z = np.stack([Z1, Z2], axis = 1)\n",
    "        \n",
    "    return E_choice, Env, Y, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2147f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_latent_3d(df, m):\n",
    "    # creating figure\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    # creating the plot\n",
    "    plot = ax.scatter(df['Z1'], df['Z2'], df['Z3'], c=m)\n",
    "    # setting title and labels\n",
    "    ax.set_title(\"3D plot\")\n",
    "    ax.set_xlabel('Z1-axis')\n",
    "    ax.set_ylabel('Z2-axis')\n",
    "    ax.set_zlabel('Z3-axis')\n",
    "\n",
    "    # displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcbce6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2164/470419312.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mNet1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_output):\n",
    "        super(Net1, self).__init__()\n",
    "        torch.manual_seed(2)\n",
    "        # hidden layer\n",
    "        self.W_1 = Parameter(\n",
    "            init.xavier_normal_(torch.Tensor(num_hidden, num_features))\n",
    "        )\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
    "        # output layer\n",
    "        self.W_2 = Parameter(\n",
    "            init.xavier_normal_(torch.Tensor(num_output, num_hidden))\n",
    "        )\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "        # define activation function in constructor\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_output):\n",
    "        super().__init__()\n",
    "        self.function = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=num_hidden, out_features=num_output),\n",
    "        )\n",
    "    def forward(self, Z):\n",
    "        return self.function(Z)\n",
    "\n",
    "\n",
    "def generate_x_from_z(env, net):\n",
    "    \"\"\" Runs neural network on latent variables to return X\"\"\"\n",
    "\n",
    "    return net(torch.from_numpy(env[\"Zs\"].astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e31b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal_dist(Distribution):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.c = 2 * np.pi * torch.ones(1).to(self.device)\n",
    "        self._dist = dist.normal.Normal(torch.zeros(1).to(self.device), torch.ones(1).to(self.device))\n",
    "        self.name = 'gauss'\n",
    "\n",
    "    def sample(self, mu, v):\n",
    "        with torch.no_grad():\n",
    "            eps = self._dist.sample(mu.size()).squeeze()\n",
    "        scaled = eps.mul(v.sqrt())\n",
    "        return scaled.add(mu)\n",
    "\n",
    "    def log_pdf(self, x, mu, v, reduce=True, param_shape=None):\n",
    "        \"\"\"compute the log-pdf of a normal distribution with diagonal covariance\"\"\"\n",
    "        if param_shape is not None:\n",
    "            mu, v = mu.view(param_shape), v.view(param_shape)\n",
    "        lpdf = -0.5 * (torch.log(self.c) + v.log() + (x - mu).pow(2).div(v))\n",
    "        if reduce:\n",
    "            return lpdf.sum(dim=-1)\n",
    "        else:\n",
    "            return lpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16834f1e",
   "metadata": {},
   "source": [
    "## VAE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class UglyVAE(nn.Module):\n",
    "    def __init__(self, x, y, e, input_size, output_size, input_size_de, output_size_de, input_size_pr, output_size_pr, beta=1):\n",
    "        super().__init__()\n",
    "        # prior_params\n",
    "        self.prior_mean = torch.zeros(1).to('cpu')\n",
    "        self.logl = torch.zeros(1).to('cpu')\n",
    "        \n",
    "        # Encoder\n",
    "        self.func_en = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=2*output_size)\n",
    "            )\n",
    "        \n",
    "        # Decoder\n",
    "        self.func_de = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size_de, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=output_size_de)\n",
    "            )\n",
    "        \n",
    "        # Prior\n",
    "        self.func = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size_pr, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=output_size_pr*2)\n",
    "            )\n",
    "        \n",
    "        self.normal_dist = Normal_dist()\n",
    "        \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = self.normal_dist.sample(mu, std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def prior_params(self):\n",
    "        return self.prior_mean, self.logl.exp()\n",
    "    \n",
    "    def forward(self, x, y, e, beta=1):\n",
    "        # Format input\n",
    "        xye=torch.cat((x, y, e), 1)\n",
    "        \n",
    "        # Get prior params\n",
    "        prior_params = self.prior_params()\n",
    "        \n",
    "        # Get encoder params\n",
    "        z_usigma = self.func_en(xye)\n",
    "        zu, zsigma = z_usigma.chunk(2, dim = -1) # Get mu and sigma\n",
    "        \n",
    "        #zsigma = zsigma.exp()\n",
    "        encoder_params = zu, zsigma\n",
    "\n",
    "        # Sample Z\n",
    "        z = self.reparametrize(zu, zsigma)\n",
    "\n",
    "        # Get decoder params\n",
    "        de_u = self.func_de(z)\n",
    "        \n",
    "        return zu, zsigma, de_u, z, prior_params\n",
    "\n",
    "    \n",
    "    def elbo(self, x, y, e, beta=1):\n",
    "        zu, zsigma, de_u, z, prior_params = self.forward(x, y, e)\n",
    "        # Samples from distributions with the acquired parameters\n",
    "        lvar = 0.01*torch.ones(1).to('cpu')\n",
    "        log_px_z = self.normal_dist.log_pdf(x, de_u, lvar) # p(x | u, sigma)\n",
    "        log_qz_xy = self.normal_dist.log_pdf(z, zu, zsigma.exp())\n",
    "        log_pz_ye = self.normal_dist.log_pdf(z, *prior_params)\n",
    "\n",
    "        # And compute kl and elbo\n",
    "        kl = -log_qz_xy + log_pz_ye\n",
    "\n",
    "        elbo = log_px_z + beta*kl\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        return elbo.mean()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf70649",
   "metadata": {},
   "source": [
    "## iVAE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class UglyiVAE(nn.Module):\n",
    "    def __init__(self, x, y, e, input_size, output_size, input_size_de, output_size_de, input_size_pr, output_size_pr, beta=1):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.func_en = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=2*output_size)\n",
    "            )\n",
    "        \n",
    "                # Decoder\n",
    "        self.func_de = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size_de, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=output_size_de)\n",
    "            )\n",
    "        \n",
    "                # Prior\n",
    "        self.func = nn.Sequential(\n",
    "                nn.Linear(in_features=input_size_pr, out_features=6),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=6, out_features=output_size_pr*2)\n",
    "            )\n",
    "        \n",
    "        self.normal_dist = Normal_dist()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x, y, e, beta=1):\n",
    "        # Format input\n",
    "        xye=torch.cat((x, y, e), 1)\n",
    "        \n",
    "        # Get encoder params\n",
    "        z_usigma = self.func_en(xye)\n",
    "        zu, zsigma = z_usigma.chunk(2, dim = -1) # Get mu and sigma\n",
    "        zsigma = zsigma.exp()\n",
    "        encoder_params = zu, zsigma\n",
    "\n",
    "        # Sample Z\n",
    "        z = self.normal_dist.sample(zu, zsigma)\n",
    "\n",
    "        # Get decoder params\n",
    "        de_u = self.func_de(z)\n",
    "        \n",
    "        # Get prior params\n",
    "        ye = torch.cat((y, e), 1)\n",
    "        prior_u, prior_sigma = self.func(ye).chunk(2, axis = -1)\n",
    "        prior_sigma = prior_sigma.exp()\n",
    "        \n",
    "        return zu, zsigma, de_u, prior_u, prior_sigma, z\n",
    "    \n",
    "    def elbo(self, x, y, e, beta=1):\n",
    "        zu, zsigma, de_u, prior_u, prior_sigma, z = self.forward(x, y, e)\n",
    "        # Samples from distributions with the acquired parameters\n",
    "        lvar = 0.01*torch.ones(1).to('cpu')\n",
    "        log_px_z = self.normal_dist.log_pdf(x, de_u, lvar) # p(x | u, sigma)\n",
    "        log_qz_xye = self.normal_dist.log_pdf(z, zu, zsigma)\n",
    "        log_pz_ye = self.normal_dist.log_pdf(z, prior_u, prior_sigma)\n",
    "\n",
    "        # And compute kl and elbo\n",
    "        kl = -log_qz_xye + log_pz_ye\n",
    "\n",
    "        elbo = log_px_z + beta*kl\n",
    "\n",
    "        return elbo.mean()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EnvDataset(Dataset):\n",
    "    def __init__(self, X, Y, E):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X)\n",
    "        self.Y = torch.tensor(Y).unsqueeze(1)\n",
    "        self.E = torch.nn.functional.one_hot(torch.tensor(E).long())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index], self.E[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def MCC(true_z, predicted_z):\n",
    "    \"\"\"Calculates the Correlation Coefficient between all pairs of true \n",
    "    and recovered latent variables for one environment \n",
    "\n",
    "    Uses Pearsons Corr Coef\n",
    "\n",
    "    from paper: \n",
    "    We also compute the mean correlation coefficient (MCC) used in Khemakhem et al. (2020a), which\n",
    "    can be obtained by calculating the correlation coefficient between all pairs of true and recovered\n",
    "    latent factors and then solving a linear sum assignment problem by assigning each recovered latent\n",
    "    factor to the true latent factor with which it best correlates\n",
    "\n",
    "    Args:\n",
    "        true_z (numpy array): 2D dimensional numpy array, where columns represent variables\n",
    "        predicted_z (numpy array): _description_\n",
    "    \"\"\"\n",
    "    num_true = len(true_z[0])\n",
    "    num_predicted = len(predicted_z[0])\n",
    "    corr_matrix = np.corrcoef(true_z, predicted_z, rowvar=False)\n",
    "    reduced_matrix = corr_matrix[\n",
    "        0:num_true, num_true : len(corr_matrix[0]) + 1\n",
    "    ]  # where rows are true and columns are predicted\n",
    "    row_ind, col_ind = linear_sum_assignment(reduced_matrix)\n",
    "\n",
    "    mcc = [reduced_matrix[row_ind[i], col_ind[i]] for i in range(len(row_ind))]\n",
    "    print(mcc)\n",
    "    mcc = np.sum(mcc) / (num_predicted + num_true)\n",
    "    return mcc\n",
    "\n",
    "\n",
    "def plot_MCC(mcc_model, mcc_mean, mcc_var):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        mcc_model (list): names of models that MCC was performed on\n",
    "        mcc_mean (list): the returned value of the MCC function\n",
    "        mcc_var (list): the variance that corresponds with the mean values given above\n",
    "    \"\"\"\n",
    "    plt.bar(mcc_model, mcc_mean, yerr=mcc_var)\n",
    "\n",
    "\n",
    "# test MCC\n",
    "#random.seed(10)\n",
    "#true_z = np.random.rand(4, 3)\n",
    "#test_z = np.random.rand(4, 3)\n",
    "## print(f\"true z: {true_z}\")\n",
    "#print(MCC(true_z, test_z))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
